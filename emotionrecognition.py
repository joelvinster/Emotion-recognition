# -*- coding: utf-8 -*-
"""EmotionRecognition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11FwdFXiW5RxU2VJ2Q97RWQEmkJY6rj-C
"""

!pip install librosa numpy pandas torch torchaudio transformers datasets

import torchaudio
import librosa
import numpy as np
import pandas as pd

# Load an audio file
file_path = "/content/03-01-08-02-02-02-15.wav"
y, sr = librosa.load(file_path, sr=None)

# Extract MFCC features
mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)

def extract_features(file_path):
    y, sr = librosa.load(file_path, sr=None)
    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)
    return np.mean(mfccs.T, axis=0)

import torch
import torch.nn as nn
import torch.optim as optim

class EmotionModel(nn.Module):
    def __init__(self):
        super(EmotionModel, self).__init__()
        self.fc1 = nn.Linear(40, 128)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 3)  # Happiness, Anger, Sadness

    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        return self.fc3(x)

model = EmotionModel()

from torch.utils.data import Dataset, DataLoader
import torch

class EmotionDataset(Dataset):
    def __init__(self, file_paths, labels):
        self.file_paths = file_paths
        self.labels = labels

    def __len__(self):
        return len(self.file_paths)

    def __getitem__(self, idx):
        features = extract_features(self.file_paths[idx])
        label = self.labels[idx]
        return torch.tensor(features, dtype=torch.float32), torch.tensor(label, dtype=torch.long)

# Example: File paths and corresponding labels
file_paths = ["/content/03-01-08-02-02-02-15.wav", "/content/03-01-07-01-01-02-15.wav", "/content/03-01-08-02-02-02-15.wav"]  # Replace with actual paths
labels = [0, 1, 2]  # Example: 0=Happy, 1=Angry, 2=Sad

dataset = EmotionDataset(file_paths, labels)
train_loader = DataLoader(dataset, batch_size=8, shuffle=True)

def predict_emotion(file_path):
    features = torch.tensor(extract_features(file_path)).unsqueeze(0)
    output = model(features)
    emotion = torch.argmax(output, dim=1)
    return ["Happy", "Angry", "Sad"][emotion]

print(predict_emotion("/content/test_audio.wav"))